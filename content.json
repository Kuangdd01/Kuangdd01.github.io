{"pages":[{"title":"关于我","text":"关于本人本人：男 昵称：咚咚 联系我：992309340@qq.com 爱好：乒乓球、羽毛球、电影 感兴趣的方向：自然语言处理，数据挖掘 ​ 学不会的东西：算法，数学 天赋技能：摆烂 个人特点：间歇性热血，持续性无所事事 ​ 感叹：Rebecca 和 Lucy 都好好QWQ Best Lucy T-T","link":"/about/index.html"},{"title":"我的观看动漫发癫日记","text":"edc49dcbc009f894787266a153d2d10afc04487696bf50b3a91c8938186f39c031049dcc46f026e2e82d9f897bb07e975dc739240baac69d9d25459ea24c162de018cd212627d5c0e56e87a55864c08a2679e1a7d81480c619902d4175ba236f0fd7c52d9050d02bc542a17ceeab9e33e6d1d0a6637033775b51975e9c39bd64e4fee3fa75e4de34c5dc961b48f03f399592846f283d33a75d082c87242db300ad49d665a746e2a654a7a0fd24efad97367a1f08aacd5edeafabc14f663c3a9640354f16f69159c27eb9ffd74e2565d1193fb861c1fa01d4563fc92c2a1bc5f10582a7694a1c1cf60befae0ac1ac19b7f48789fa444579dbbc4351f1525bfbf7b1ef8a268c827b6ceb8e8df013148cb2aa16691b9fd5e4c19c40042b835b810fd1b2d2ffa65fa957e0c61d17ae7984051d1314bcc9416263b3d1174a99b3110fdcf0c0c43b08ac29eb3e59ce05b6e4444224d57ea17981e2655dae5d3fc7510b8fbd43156ee126ca3ed3e2e98672d2a9dbf2479a79b779908fec525643f1458a922382b890bbf3c1391a632ac246d98c44a6a0b587ca7f093ca588985ad6dfd6a497fba89fa1cdc57e11db18f93b334fb0aa64c35f34e0e986123a69288f107fb500abd9d75b26ed996536813f019de417c08f8a7eb94086064ce9acbe4af7b6aaac0a814b86166d8592038585f57961681b288935811955da3edd769d985fb0ee86f9938c2aa61dcd0817db58633c482eab370ba6b18f06dfc8045cb67b5b9f86fe4f0a9f81e8d9b0eabdaaec1110998f74a669ab0631872a556665c5ede3ed63828b443b87d6385e4ab61dd8b82b934e91de5bef22d724662486aac3e7aeedf8658484f1609d044c600b75d28aa150040cb1b0607ed1110953d8c4872c7c9be05373d8eb0125a7eab495ea224a4a438b5ae8d094bd4b09ddb06a4186ba56d31876ef904d859283184b51a626234a32cf07964ba524cde2474f278ad9564466bf7f69a6404ec9e28051a669429998ad615f8be4b1157d5b5cf49f56c615764f1298c5ab7d916096137608b55da0a714b2b13616b2c3538dd179e62cd7a82ed4bbd8bad229c414e36c1a323980ae327cfd5d61b9b360056d3fad2ca5bc9329a81c0dfb09e95de4b55f6ab6600454784ce5b7c46ea2db54d1449e7bf844c035e6 这里需要密码才能访问。","link":"/anime/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"相册","text":"21B的天空–before June 3rd 手机随拍","link":"/photos/index.html"}],"posts":[{"title":"about_site","text":"关于本站 学习记录 激励自己向一些大佬学习","link":"/2023/01/07/about-me/"},{"title":"cs224n","text":"cs224n课程学习记录课程相关资源video：b站视频链接 coursework：官网链接 lecture1-Word Vectors and Introductionkey methods： RNN, transformer, attention ability to use pytorch one-hot vector不能提更单词相关性操作，并且向量维度会指数级增长 现代向量编码，分布式语义：一个词的意义由它附近出现的的词语来定义/表示，上下文表示（context ） 1.word embeddings分布式语义对word classification具有重要贡献:) 2.Word2vec中心词和上下文词出现的概率计算模型 目标函数： 定义一个滑动窗口大小为$2M$，预测上下文词组中的数据似然(data likehood)$$L(\\theta)=\\prod_{t=1}^{T} \\prod_{-m\\le{j}\\le{m}}P{(w_{t+j}\\ | w_t;\\theta)}$$T表示语料中心词的变化，其中$\\theta$是可变量，可以训练至似然最大 与此同时loss function:$$J(\\theta) = -\\frac{1}{T}\\rm{log}L(\\theta)=-\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-m\\le{j}\\le{m}}\\rm{log}P(w_{t+j}|w_t;\\theta)$$对于概率函数$P(w_{t+j}|w_t;\\theta)$的计算： 我们用两对词向量对于每个词w有$v_w$，$u_w$两种表示，前者表示w为中心词的表示后者为上下文的表示于是我们有对于每个中心词c和上下文若干词汇o有以下公式：$$P(o|c) = \\frac{\\rm{exp}(u_o^Tv_c)}{\\sum_{w\\in{V}}\\rm{exp}(u_w^Tv_c)} \\to{P(w_{t+j}|w_t;\\theta)}$$ -&gt;make our word vector 其中 course Q&amp;A未记录 _(:з」∠)_有空就记录 词汇的多意义问题对wordvector，直接融合叠加 lecture2-Neural Classifiers1.词袋模型模型学习阶段将含义相近的单词放置在高纬度向量空间中临近位置，按词义分组。 To学习好的词向量 从random vectors开始，通过不断调整$\\theta$的值来使得$J(\\theta)$的值最小 equation：$\\theta^{new} = \\theta^{old}-\\alpha\\nabla_{\\theta}J(\\theta)$，但是在计算损失函数的代价会随着corpus的增加而快幅度增加单个梯度增加缓慢，使用随机梯度下降，只使用一小批梯度估计，小部分梯度更新会使得结果充满噪声和误差，少部分中心词更新。并且每次梯度下降只更新窗口内词汇参数，窗口外词汇透明（稀疏更新）。 随机梯度下降有噪声，可能会体现出有反弹的趋势，但在复杂网络中可以比普通梯度下降中更快更好，反直觉。 word2vector including:skip-grams(SG)、CBOW k-negative sampling最开始使用的不是softmax而是k负采样方法来作为$\\rm{J(\\theta)}$损失函数为此时为求J max：$$J_t(\\theta) = \\rm log(u_o^Tv_c)+\\sum_{i=1}^{k}E_{j\\sim P(w)}[log \\sigma(-u_j^Tv_c) ] \\tag{NS} \\\\ J(\\theta) = \\frac{1}{T}\\sum^T_{t=1}J_t(\\theta) \\\\ \\sigma(x) = \\frac{1}{1 + e^{-x}}$$解释损失函数： 在每个词滑动窗口移动过程中窗口外词汇即负样本点过多，负样本太多，会导致训练样本失衡，同时负样本太多会导致数据量太大难以训练、以及训练模型有偏。所以减少负样本点的抽取和训练，采用K个负样本点的方. softmax计算量大,分母位置有许多$e^{-x}$等待求值，直接用$\\rm log{\\sigma(x)}$的方式来取得probity. 转化为最小形式$\\rm J_{neg-sample}=-log(\\sigma(u_o^Tv_c) - \\sum _\\limits{k\\in {K-sample indices}}log\\sigma(-\\textbf{u}_k^T\\textbf{v}_c )$ 课程中所讲述的vector analogies以及词向量关系和co-occur不多做赘述。 多义词在词向量中的表现： lecture3-Backprop and NN","link":"/2022/10/02/cs224n/"},{"title":"力扣笔记","text":"第一周哈希表在连续数组中的应用相关例题： 12345678910111213推荐按顺序做。1. 两数之和 √560. 和为 K 的子数组 √974. 和可被 K 整除的子数组 √1590. 使数组和能被 P 整除 √523. 连续的子数组和 √525. 连续数组面试题 17.05. 字母与数字1915. 最美子字符串的数目930. 和相同的二元子数组1371. 每个元音包含偶数次的最长子字符串1542. 找出最长的超赞子字符串 1.统计美丽子数组数目 6317原题： 给你一个下标从 0 开始的整数数组nums 。每次操作中，你可以： 选择两个满足 0 &lt;= i, j &lt; nums.length 的不同下标 i 和 j 。 选择一个非负整数 k ，满足 nums[i] 和 nums[j] 在二进制下的第 k 位（下标编号从 0 开始）是 1 。 将 nums[i] 和 nums[j] 都减去 2k 。 如果一个子数组内执行上述操作若干次后，该子数组可以变成一个全为 0 的数组，那么我们称它是一个 美丽 的子数组。 请你返回数组 nums 中 美丽子数组 的数目。 子数组是一个数组中一段连续 非空 的元素序列。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 朴素思想 TLEclass Solution {public: static const int N = 25; static const int M = 1e5 + 10; int s[M][N] = {0}; bool check(int l, int r) { for(int i = 1; i &lt;= 21; i ++) { int t = s[r][i] - s[l - 1][i]; if(t % 2) return false; } return true; } long long beautifulSubarrays(vector&lt;int&gt;&amp; nums) { int n = nums.size(); for(int i = 0; i &lt; n;i ++) { int num = nums[i]; for(int j = 0; j &lt;= 20; j ++) { int k = num &gt;&gt; j &amp; 1; if(k) s[i + 1][j + 1] += 1; } } for(int i = 1; i &lt;= n; i ++) for(int j = 1; j &lt;= 21; j ++) s[i][j] += s[i - 1][j]; long long res = 0; for(int i = 0;i &lt; n ;i ++) for(int j = i; j &lt; n; j ++ ) if(check(i + 1, j + 1)) res ++; return res; }};//利用哈希表来寻找左侧出现过的值 * 降低时间复杂度class Solution {public: long long beautifulSubarrays(vector&lt;int&gt;&amp; nums) { int n = nums.size(); vector&lt;int&gt; s(n + 1); //异或和 通过异或来简化统计二进制下不同位1个数的奇偶性 for(int i = 1; i &lt;= n ; i ++) { s[i] = s[i - 1] ^ nums[i - 1]; } unordered_map&lt;int, int&gt; h; long long res = 0; for(auto c: s) { cout &lt;&lt; c &lt;&lt;endl; // 此处表示异或和下的间隔，先+=再++ res += h[c]++; } return res; }}; 2.使数组和能被 P 整除 1590原题： 给你一个正整数数组 nums，请你移除 最短 子数组（可以为 空），使得剩余元素的 和 能被 p 整除。 不允许 将整个数组都移除。 请你返回你需要移除的最短子数组的长度，如果无法满足题目要求，返回 -1 。 子数组 定义为原数组中连续的一组元素。$$找到i,j满足：S[i + 1:j] % p=0\\有前缀和性质使得(S[j]-S[i])%p=SUM%p$$ 1234567891011121314151617181920212223242526272829303132class Solution {public: int minSubarray(vector&lt;int&gt;&amp; nums, int p) { int n = nums.size(); int k = 0; for(auto c : nums) { k = (k + c) % p; // cout &lt;&lt; k &lt;&lt;endl; } if(!k) return 0; int ans = n; unordered_map&lt;int, int&gt; h; int y = 0; //坐标保持一致 h[0] = -1; for(int i = 0; i &lt; n;i ++) { h[y] = i; y = (y + nums[i]) % p; if(h.count((y - k + p) % p)) { //注意idx坐标位置、举例说明 int idx = h[(y - k + p) % p] - 1; if(ans == -1 || ans &gt; i - idx) ans = i - idx; } } //表示如果全部删除才能满足结果 返回-1 return ans == n ? -1 : ans; }}; 3.连续的子数组和 523原题： 给你一个整数数组 nums 和一个整数 k ，编写一个函数来判断该数组是否含有同时满足下述条件的连续子数组： 子数组大小 至少为 2 ，且 子数组元素总和为 k 的倍数。 如果存在，返回 true ；否则，返回 false 。 如果存在一个整数 n ，令整数 x 符合 x = n * k ，则称 x 是 k 的一个倍数。0 始终视为 k 的一个倍数。 注意n可以为0 代码： 12345678910111213141516171819202122232425262728class Solution {public: bool checkSubarraySum(vector&lt;int&gt;&amp; nums, int k) { int n = nums.size(); unordered_map&lt;int, int&gt; h; int s = 0; //特殊判断 if(n &lt; 2) return false; //坐标偏移 h[0] = -1; for(int i = 0; i &lt; n;i ++) { // h[s] = i; s = (nums[i] + s) % k; if(h.count(s)) { int idx = h[s]; if(i - idx &gt;= 2) return true; } else //可能存在有0的情况使得连续出现的s相同于是h[s]的值异常更新，使得最终长度&lt;2；于是此处特殊判断存在s就跳过更新哈希表 （本题只需要判断1-0不需要长度所以简便即可） { h[s] = i; } } return false; }}; 小结：使用哈希表保存前缀和或者前缀XX数组中的部分状态，需要匹配则通过O(1)的count/find函数返回，降低时间复杂度","link":"/2023/03/12/leetcode%E7%AC%94%E8%AE%B0/"},{"title":"nlp小知识","text":"一些知乎看到的小知识点0.word、subword和character在神经机器翻译中，通常有一个固定的词表，并且模型的训练和预测都非常依赖这个词表。在神经网络的训练过程中，需要对词表中每个词做向量表，每个词对应不同的向量，即embedding的过程。每个不同的词对应不同的向量，即使两个词看起来十分相近，但在训练过程词向量没有任何关系。这就导致一个单词因为拥有不同的形态产生不同的词，从而产生大词汇量的问题。 机器翻译的词表是定长的，但是需要实际翻译的词汇是开放的(out of vocabulary)。以前的做法是新词汇添加到词典中，但是过大的词典会带来两个问题： 稀疏问题: 某些词汇出现的频率很低，得不到充分的训练 计算量问题: 词典过大，也就意味着embedding过程的计算量会变大 同时，这种word-level的处理方式并不能通过增大词表真正解决OOV的问题，因为再大的词典不能真正覆盖所有的词汇。 为了处理这个问题，一个思路是将字符当做基本单元，建立character-level模型。character-level模型试图使用26个字母加上一些符号去表示所有的词汇，相比于word-level模型，这种处理方式的粒度变小，其输入长度变长，使得数据更加稀疏并且难以学习长远程的依赖关系。类似的工作可参考Character-Level Neural Machine Translation，实验结论是基于字符的模型能更好处理OOV问题，并且能更好学习多语言之间通用的语素。 word-level模型导致严重的OOV，而character-level模型粒度又太小，那么subword-level的处理方式就应运而生。subword将单词划分为更小的单元，比如”older”划分为”old” 和 “er”，而这些单元往往能应用到别的词汇当中。举个例子： 123训练集的词汇: old older oldest smart smarter smartest``word-level 词典: old older oldest smart smarter smartest 长度为6``subword-level 词典: old smart er est 长度为4 将词划分成字词的形式，能够大大降低词典的大小。同时，未知词汇能以subword组合的形式表示出来，也能提升词典的表达能力。 1.subword算法1.1 Byte Pair EncodingBPE是一种数据压缩的方式，它将字符串中最常见的一对连续字符数据替换成该字符串中不存在的字符串，后续再通过一个词表重建原始的数据。BPE的处理过程可以理解为一个单词的再拆分过程。如”loved”,”loving”,”loves”这三个单词，其本身的语义都是”爱”的意思。BPE通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。 流程： 准备预料-分解为最小单元-例如26字母+符号作为原始词表 根据语料统计相邻字符对出现频次 挑选出相邻频次最高的字符对，例如：’a’,’b’-&gt;’ab’加入词表，训练语料中所有该相邻字符对都融合 repeat2,3直到字符表数量达到期望or再次合并后会出现对出现频率-&gt;1 编码-解码： 得到词表后对词表按照字词长度由大到小排序。编码时对于每个单词，遍历排好序的字词词表寻找是否有token是当前单词的子字符串，如果有则该token是表示单词的token之一。从最长的token迭代至最短token，尝试将每个单词的子字符串替换为此表中的token，如果还有没被替换的子字符串则用特殊token代替例如&lt;unk&gt;，解码逆过程不表 1.2 wordpiecebert使用分词形式，与1.1BPE相似，但是基于概率生成，作者原话： 1Choose the new word unit out of all possible ones that increase the likelihood on the training data the most when added to the mode. 公式理解：$$logP(sentence)=\\sum\\limits_{i=1}^{n}logP(t_i) \\ \\ if \\ Sentence \\ has \\ token \\ t1…t_n$$对于一个x，y相邻的token合并为z，整个句子的似然变化如下式：$$\\delta_p=logP(t_z)-(logP(t_x)+logP(t_y))=log(\\frac{P(t_z)}{P(t_x)P(t_y)})$$此式说明wordpiece的合并token相似但不相同，wordpiece和BPE的核心区别就在于wordpiece是按token间的互信息来进行合并而BPE是按照token一同出现的频率来合并的。停止可以指定$\\delta_p$增量小于某一阈值时停止。 1.3 unigram language model首先，了解一下怎么样在给定词表的条件下最大化句子的likelihood。 给定词表及对应概率值: {“你”:0.18, “们”:0.16, “好”:0.18, “你们”:0.15}，对句子”你们好“进行分词: 划分为”你@@” “们@@” “好” 的概率为 0.18 *0.16 *0.18=0.005184 划分为”你们@@” “好@@” 的概率为 0.15*0.18=0.027 明显看出后一种分词方式要比前一种好，当然在真实的案例下词表可能有几万个token，直接罗列各种组合的概率显然不可能，所以需要用到viterbi（维特比）算法。因此在给定词表的情况下，可以 1.计算每个token对应的概率；2.找到一个句子最好的分词方式","link":"/2022/11/01/nlp%E5%B0%8F%E7%9F%A5%E8%AF%86/"},{"title":"组会","text":"","link":"/2022/10/21/%E7%BB%84%E4%BC%9A/"},{"title":"保研底端废物菜狗-保研面经","text":"1.个人情况个人情况：东北末2 计算机专业 学科评估B+ 排名：夏令营rk6/250 预推免rk4/248 竞赛：数模省奖*2 相当于无 科研：$0$ 六级：$556$ 最终去向：$北航六系电子信息$ 夏令营情况： 院校 入营 优营 北理计算机硕 0 0 北航计算机硕 0 0 中南计算机硕 1 1 哈工大计算机硕 1 0 吉林大学人工智能硕 1 0 南京大学计算机硕 0 0 人大信院 1 0 四川大学计算机学院硕 0 0 华南理工大学计算机硕 1 1 中山大学计算机硕 0 0 天津大学计算机硕 0 0 厦门大学人工智能硕 0 0 HIT-scir 1 1 预推免情况： 院校 入营 优营 北理计算机硕 0 0 北航计算机硕 1 1 华东师大大数据硕士 1 0 华东师大计算机硕士 0 0 华中科技大学计算机硕士 0 0 武汉大学计算机硕士 0 0 武汉大学网安硕士 0 0 中山大学计算机硕士 1 候补-补到 浙江大学软件学院 1 0 同济大学计算机硕士 0 0 2.夏令营情况总的来说入营情况全看rk前列大佬给不给饭吃 o(╥﹏╥)o 入营情况非常惨淡并且为数不多的参营也惨淡收场 2.1中南大学中南大学大海无量 逃~，中南本次推免招生还是采用广撒网战术，入营点击就送，宣讲全看自觉，最后大概疯狂超发，面试考核五分钟，纯背景面，介绍了一下大概情况被老师中途打断换成英语自我介绍，流水线式单人面试。 2.2哈工大计算学部 HRB只能说到最后还是HIT o(╥﹏╥)o 我是精神hiter,C9唯一 一个入营机会是工大给的，砰砰砰，%%% orz。 参营过程开摄像头 调整虚拟背景 罚坐2天全程听宣讲，线上参营过程中，我最兢兢业业的一个营了，听完宣讲感觉计算学部每个中心都很厉害，cv左神坐镇, IEEE fellow的物联网，nlp强组scir、机翻，还有好多厉害的组，菜鸡的我只觉得非常厉害T.T，一直把hit作为自己的dream school，经过了几天的宣讲和安排，唯一的体验就是哈工大的老师都非常好，也非常积极与学生进行沟通，可以在夏令营期间联系感兴趣的老师，BUT 教务老师极其冷漠或者机械，夏令营最后的安排到了宣讲当晚才通知，参营体验很糟糕，在这一点上对比鲜明了，并且并未解释不可二次面试的要求，不回复任何学生的问题，这在Hit其他院系都是有明确通知的，个人感受是hit的教务老师配不上hit的各个中心的老师。 考核分为：面试80+机试20 面试：结构化面试，分为几个部分的考核。 0.开放问题 1.心智测试 2.专业知识面试 3.竞赛/项目检查 4.情景题考核 总的来说考核非常全面也非常高效，感谢当时的志愿者学长和老师，奈何本人当时准备并不是很充分，对专业知识掌握度没有达到足够的要求，全然忘记了SQL语句的顺序（group by &amp; where）,于是理所应当的没有拿到优营T.T 机试：20， C语言期末上机考试难度 可能还要低好多 服务器炸穿，不是在线评测，问了同学说这是HITER C语言期末考试的系统，但是开局200人进网页系统，我直接进不去并且网页卡崩 乐），最后紧急下载client客户端登录，还是卡的一···，于是做完编程题两项，改错题直接懒得做了（实在太卡了，下一页点击无反应），交一发直接溜了，室友说可以看到评测的分数的，但是机试在晚上，我交了直接没管了。 结果：一直没有反应结果的消息，在某天绿裙有人说了句有人收到邮件吗？ 苦等没收到,遂寄。 据说优营 + 候补 == 100 2.3吉林大学人工智能jlu当时看到人工智能院老师都很年轻,看起来很厉害就兴致勃勃的冲了。 考核：面试 内容：西瓜书部分章节 过程：进钉钉会议抽题面试，英语有翻译题和朗读, 专业题由于不是科班的并且临时抱佛脚，考的有两个知识点不知道疯狂道歉··· 结果：寄 无wl 2.4人大信院王老吉属于最早投出简历的学校，看到佬那么多都冲了当时以为无望了┭┮﹏┭┮，最后入营结果开的很晚，入营了还是很兴奋的，Ruc专，鼠鼠的白月光。 考核：机试100 + 面试200 机试：刷人 + 算分，机试使用人大自己的系统，反馈速度还挺快的，难度适中，据说一路打暴力能刷到80分，但是鼠鼠水平太差了，只有78分，按过点给分 面试：非常正式，英语自我介绍 + 英语问答 + 抽题专业课问答，小插曲是不知道我为啥抽到一题工商管理类的题目，完全没听过直接道歉, 其实体感答得还可以，特意准备了关于db的知识和问答，但是还是寄，大概是因为自己pass了一道题目。 结果：寄 + 不在wl 2.5华南理工计算机最开始甚至没入营，但是巧的是当天tju, tongji，buaa都在当天开营，于是有很多大佬放弃，被补录入营，小姐姐电话通知，一直以为是广告，挂了好多次hhhhh，最后与教秘联系上，遂入营，恰好鼠鼠当时也没有其他学校要，就去参加scut的宣讲和面试了，不得不说地处广州背靠深圳这种得天独厚的优势让scut的就业不愁，很香~ 考核：面试 面试：英语问答 + 3名老师10分钟拷打 被老师拷打的自己的水项目，和一些领域相关问题，答得磕磕绊绊，还指出了ppt里顶会缩写写错了，被老师怼了半天，说我是在抖音上学的deep learning.. 我只能疯狂道歉，最后以为寄了，所以就没看后续，后续被教秘联系说可以去联系老师，并发了老师的邮箱，联系了两位老师，第二位老师简单面试了一下基本情况,通过，至此scut算是夏令营第一个也是唯一一个offer了 o(╥﹏╥)o 这就是纯rk废物的结局了 ···预推免未完待续··· 3.预推免情况有点懒得写了 ····","link":"/2022/10/13/%E4%BF%9D%E7%A0%94%E9%9D%A2%E7%BB%8F/"},{"title":"开发类学习","text":"啥都没有····","link":"/2022/10/12/%E5%BC%80%E5%8F%91%E7%B1%BB%E5%AD%A6%E4%B9%A0/"},{"title":"408","text":"一、计算机网络1.计算机网络体系结构协议由语法、语义和同步三部分组成。 语法：规定数据格式 语义：规定所要完成的功能 同步：规定完成的条件、时序关系。 接口是相邻两层交换信息的节点，逻辑概念。 服务是指下层为紧邻的上层提供功能调用，垂直功能。 2.ISO/OSI &amp; TCP/IPOSI参考模型：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。其中，低三层称为通信子网，高三层为资源子网。 1.物理层：（规定了一些物理参数和电气特性） 为数据端设备透明地传输原始比特流。 2.数据链路层： 帧，将网络层传来的IP数据报组装成帧。 封装成帧、差错控制、流量控制、传输管理etc。 3.网络层：数据报 任务：把网络层数据单元从源地址传到目的端，包括路由选择、流量控制、拥塞控制、差错控制、网际互联etc。 4.传输层：tcp/udp 负责主机两个进程之间的通信，提供端到端的可靠传输服务，端到端的服务质量控制。 difference：数据链路层提供点到点，传输层是端到端。 5.会话层： 为表示层实体和用户建立同步 6.表示层： 不同机器编码、表示方式的变化层 7.应用层： 提供不同的网络应用要求，例如WWW、HTTP、FTP服务 tcp/ip：结构如图所示 3.物理层 4.数据链路层 二、数据库原理1.B树又称多路平衡查找树：提高磁盘查找效率 非叶节点： [n P0 K1 P1 K2 P2 ... Kn Pn]k表示为关键字、p表示为指针，Pi-1所指向的子树关键字均小于Ki，Pi所指向的子树的关键字均大于Ki 另：所有的叶节点都出现在同一层次上，并不带任何信息$$对于n个关键字，阶数为m，高度为h的B树有以下性质 \\log_m{(n+1)} \\leq h \\leq log_{[m/2]}((n+1)/2)+1$$B树的插入、删除： 插入： 1）定位 查找插入该关键字的位置，最底层非叶子节点 2）插入 插入后不会违反定义，插入后结点关键字个数在属于合法区间 删除：终端节点 ||非终端节点 1） 直接删除（不破坏定义） 2）借兄弟节点，并修改双亲节点 3）与兄弟节点合并 4） 删除非终端节点：1.先与终端节点替换再删除 2.先合并子树再执行删除 chapter1.绪论1、数据、数据库DB，数据库管理系统DBMS,数据库系统DBS，数据库管理员DBA。 2、DBMS主要功能：数据定义、数据组织存储管理、数据操纵、数据事务管理和运行管理、数据库建立和维护、其他。 3.DBS特点：数据结构化、数据共享性高冗余度低易扩充、数据独立性（二级映像）、统一管理（安全、完整性、并发、恢复） 4.数据模型：1类-概念模型，二类-逻辑模型、物理模型 5.概念 实体、属性、码、实体型、实体集、联系 数据模型组成：数据结构、数据操作、完整性约束三部分 关系数据库的约束条件：实体完整性、参照完整性、用户定义完整性 6.DBS三级结构模式 模式（所有用户的公共数据视图）、外模式（可见视图、是模式的子集）、内模式（与数据库11对应 表示存储方式） 二级映像：外模式/模式映像、模式/内模式映像（修改映射规则） chapter2.关系数据库 选择、投影、并、差、笛卡尔积、除 基本运算 实体完整性、参照完整性、用户定义完整性：实体完整性、参照完整性必须满足 实体完整性：主属性不为NULL 参照完整性：关系关系之间的引用中，F是关系R外码，与S上Ks对应，满足或者F上每个属性值为NULL或者=S中某个主码值 chapter4.数据库安全性chapter5.数据库完整性 定义参照完整性 Foreign Key 参照完整性检查、处理（级联操作、No ACT、setNull） 断言检查 触发器（before/after） chapter6.关系数据理论 一个关系模式是五元组$R(U,D,DOM,F)$,简化为$R&lt;U,F&gt;$ 1NF每个分量是不可分的数据项 数据依赖:函数依赖/多值依赖 关系模式存在的一些问题： 数据冗余 更新异常 插入异常 删除异常 规范化： 函数依赖（平凡非平凡、完全部分） 码（候选码【主属性】、超码、主码） 范式：满足不同程度要求 2NF：没一个非主属性完全依赖于任何一个候选码 2nf异常：插入异常、删除异常、修改复杂 3NF：不存在传递依赖 BCNF：3nf修正，对任意映射$X→Y$且Y不包含于X，X必定含有码 如图： 公理系统： 自反、增广、传递 合并、伪传递、分解 闭包 模式分解 无损连接、保持函数依赖 分解步骤 chapter7.数据库设计 需求分析设计 概念结构设计 逻辑结构设计 物理结构设计 数据库实施阶段 数据库运行和维护阶段 chapter10.数据库恢复技术 事务概念 开始、提交、回滚 ACID:原子性、一致性、隔离性、持续性 封锁协议 一级封锁协议，在事务T修改R前必须添加X锁，直到事务结束才释放。 解决丢失修改 二级封锁协议在一级上增加了T对R的读取前必须增加S锁，读完释放。解决了读脏数据 三级封锁，在一级协议基础上增加T在读取R前增加S锁，事务结束才释放。解决了读脏数据和不可重复读 可串行化调度 可串行性是并发事物正确调度的准则 三、数据结构1.红黑树性质：1.结点是红/黑色 2.根节点为黑 3.所有叶子节点都是黑色 叶子为NIL节点 4.每个红色结点的子结点都是黑色（从每个叶子到根没有连续的红色结点） 5.从一任意结点其每个叶子的所有路径黑色结点数目相同 6.从根到叶子最长路径不多于最短的可能路径的两倍长 操作：","link":"/2022/06/28/408/"},{"title":"西瓜书读书笔记","text":"chapter4-决策树4.1基本概念​ 决策树是一种常见的机器学习分类方法，在分类任务为例，在若干轮决策过程后对样本进行分类，这是一种非常自然的思考过程，以树结构来进行决策、判定。如下图所示： ​ 其中，非叶节点为属性测试，叶节点为样本分类，决策树训练的目的是为了产生一一棵泛化能力强，对未见过样本分类能力强的决策树。 4.2划分选择4.2.1信息增益​ 我们现有的信息熵公式$Ent(D)=-\\sum_\\limits{k=1}^\\limits{|y|}p_klog_2{p_k}$，其中y定义为样本分类的种类。 ​ 对于信息增益的示意是：对某个离散属性$a$在样本集合$D$上进行划分，产生$V$个分支节点，我们可以算出在$a$下进行划分的样本$D^v$的信息熵，并且根据分支节点中所包含样本数的不同给分支节点赋予权重$|D^v|/|D|$，样本数越多分支结点影响权重越大，于是计算出“信息增益”:$$Gain(D,a)=Ent(D)-\\sum_\\limits{v=1}^\\limits{V}Ent{D^v} \\tag{4.2}$$​ 一般来说，信息增益越大，说明使用属性$a$来进行划分获得的纯度提升更大(purity)，也即为分类属性的区分度高。例如著名的ID3决策树学习算法就是以信息增益为准则划分属性。 ​ 值得注意的是决策树每层划分都要基于不同的分类节点重新计算信息增益。 4.2.2增益率​ 我们时常忽略样本编号这一属性，是因为该属性不具有泛化能力，无法对新样本提供有效预测。 ​ 实际上信息增益会具有某些属性的“偏好性”，为了减少这种影响，C4.5决策树算法采用增益率来选择最优划分属性。$$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV{a}}\\tag{4.3}$$其中$$IV(a)=-\\sum_\\limits{v=1}^\\limits{V}\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|} \\tag{4.4}$$$IV(a)$称为$a$属性的固有值，属性a，可能取值数目越多，则$IV(a)$值会增大。增益率准则(未完待续)","link":"/2022/06/24/%E8%A5%BF%E7%93%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"title":"数学知识-算法","text":"一点简单数学知识1.组合数$C_a^b=\\frac{a!}{b!(a-b)!}$ $C_a^b=C_{a-1}^b+C_{a-1}^{b-1}$ 12345678910111213141516171819202122232425#include&lt;bits/stdc++.h&gt;using namespace std;const int N = 2010, mod = 1e9 + 7;int c[N][N];void init(){ for(int i = 0; i &lt; N; i ++) for(int j = 0;j &lt;= i;j ++) if(!j) c[i][j] = 1; else c[i][j] = (c[i - 1][j] + c[i - 1][j - 1]) % mod;}int main(){ inti(); int n; cin &gt;&gt; n; while(n --) { int a, b; cin &gt;&gt; a &gt;&gt; b; cout &lt;&lt; c[a][b] &lt;&lt; endl; } return 0;} 2.快速幂","link":"/2022/06/05/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86-%E7%AE%97%E6%B3%95/"},{"title":"动态规划-二进制+状态压缩","text":"动态规划：状态表示 + 状态计算状态压缩：使用整数来表示状态 二进制数字例题：acwing91 1.状态表示：$f[i,j]$ 所有从0到j，走过的所有点是i的所有路径，i为2进制数，0-1表示是否经过，表示状态$S$。 2.属性是求得$min$值 3.状态分割-状态计算， 从倒数第二个点的状态开始分割，[0,1…n -1],倒数第二个点为k, 0-&gt;k,k-&gt;j,k-&gt;j固定，求0-&gt;k的最小表示为$f[i-[j],k]$ 即得到$Min(f[i-[j],k] + a[k,j])$ 123456789101112131415161718192021222324252627282930313233#include&lt;bits/stdc++.h&gt;using namespace std;const int N = 20, M = 1 &lt;&lt; N;int f[M][N], w[N][N];int n;int main(){ cin &gt;&gt; n; for(int i = 0; i &lt; n; i ++) for(int j = 0;j &lt; n;j ++) cin &gt;&gt; w[i][j]; //初始化先假设未走通道路为无穷，待更新 memset(f,0x3f,sizeof f); f[1][0] = 0; for(int i = 0; i &lt; 1 &lt;&lt; n; i ++) for(int j = 0; j &lt; n; j ++) //此时状态要满足 if(i &gt;&gt; j &amp;&amp; 1) { for(int k = 0; k &lt; n;k ++) { if(i - (1 &lt;&lt; j) &gt;&gt; k &amp; 1) { f[i][j] = min(f[i][j],f[i - (1 &lt;&lt; j)][k] + w[k][j]); //cout &lt;&lt; f[i][j] &lt;&lt; endl; } } } cout &lt;&lt; f[(1 &lt;&lt; n) - 1][n - 1] &lt;&lt; endl;//注意运算优先级 return 0;} 树形dp例题：acwing285 建立树形结构，从根节点递归求解 分为$f[u,1]/f[u,0]$两部分求max 1.状态表示$f[u,0],$从所有以$u$为根的子树中选择，不选择$u$的集合；$f[u,1]$所有从以u为根的子树中选择，并选择$u$的集合 。 2.集合属性$Max$ 3.状态转移如上图 4.前向星建立树 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;bits/stdc++.h&gt;using namespace std;const int N = 6010;int n;int h[N], e[N], ne[N], idx;int happy[N];int f[N][2];bool has_father[N];void add(int a, int b){ e[idx] = b, ne[idx] = h[a], h[a] = idx ++;}void dfs(int u){ f[u][1] = happy[u]; for(int i = h[u]; i != -1; i = ne[i]) { int j = e[i]; dfs(j); f[u][0] += max(f[j][0], f[j][1]); f[u][1] += f[j][0]; }}int main(){ cin &gt;&gt; n; for(int i = 1; i &lt;= n ; i ++) scanf(&quot;%d&quot;,&amp;happy[i]); memset(h,-1,sizeof h); for(int i = 1; i &lt;= n - 1;i ++) { int a, b; cin &gt;&gt; a &gt;&gt; b; has_father[a] = 1; add(b, a); } int root = 1; while(has_father[root]) root ++; //找到根节点并且从根节点开始 dfs(root); cout &lt;&lt; max(f[root][0],f[root][1]) &lt;&lt; endl; return 0;} 记忆化搜索acwing901 遍历过程不可存在环，否则状态转移无法结束 和bfs思路很像 1234567891011121314151617181920212223242526272829303132333435363738394041#include&lt;bits/stdc++.h&gt;using namespace std;const int N = 310;int h[N][N], f[N][N];int n, m;int dx[4] = {1, -1, 0, 0}, dy[4] = {0, 0, -1, 1};int dp(int x, int y){ int &amp;v = f[x][y]; //此处的v引用很关键，保留了搜索过程中的最大状态，对f[][]完成修改，不重复计算 //如果使用dfs每个x,y开始都会重复搜索最大的路径 时间复杂度大 if(v != -1) return v; v = 1; for(int i = 0; i &lt; 4; i ++) { int a = x + dx[i], b = y + dy[i]; if(a &gt;= 1 &amp;&amp; a &lt;= n &amp;&amp; b &gt;= 1 &amp;&amp; b &lt;= m &amp;&amp; h[x][y] &gt; h[a][b]) v = max(v, dp(a,b) + 1); } return v;}int main(){ cin &gt;&gt; n &gt;&gt; m; for(int i = 1; i &lt;= n; i ++) for(int j = 1; j &lt;= m;j ++) scanf(&quot;%d&quot;,&amp;h[i][j]); memset(f, -1, sizeof f); int res = 0; for(int i = 1;i &lt;= n;i ++) for(int j = 1; j &lt;= m; j ++) { res = max(res, dp(i, j)); //cout &lt;&lt; dp(i, j) &lt;&lt; endl; } cout &lt;&lt; res &lt;&lt; endl; return 0;}","link":"/2022/06/03/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"title":"gallery1","text":"","link":"/2022/06/03/gallery1/"},{"title":"myProblemlist","text":"每日算法题&gt;.&lt;","link":"/2022/05/25/myProblemlist/"}],"tags":[{"name":"ML&#x2F;DL","slug":"ML-DL","link":"/tags/ML-DL/"},{"name":"专业课","slug":"专业课","link":"/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"校园风景","slug":"校园风景","link":"/tags/%E6%A0%A1%E5%9B%AD%E9%A3%8E%E6%99%AF/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"课程学习笔记","slug":"课程学习笔记","link":"/tags/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"面经","slug":"面经","link":"/tags/%E9%9D%A2%E7%BB%8F/"},{"name":"开发学习","slug":"开发学习","link":"/tags/%E5%BC%80%E5%8F%91%E5%AD%A6%E4%B9%A0/"},{"name":"组会","slug":"组会","link":"/tags/%E7%BB%84%E4%BC%9A/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/%E5%AD%A6%E4%B9%A0/"},{"name":"生活","slug":"生活","link":"/categories/%E7%94%9F%E6%B4%BB/"},{"name":"面经","slug":"面经","link":"/categories/%E9%9D%A2%E7%BB%8F/"},{"name":"开发","slug":"开发","link":"/categories/%E5%BC%80%E5%8F%91/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"}]}